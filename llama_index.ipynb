{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178cd138-d077-4076-aae3-17aff22f1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.27.tar.gz (9.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /Users/jasperhajonides/miniconda3/envs/medical_assessment/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-1.26.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m424.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m257.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.3-cp310-cp310-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.27-cp310-cp310-macosx_14_0_arm64.whl size=1833598 sha256=a171a88a367cda8345928c6a1dd539df7fe93898121bf51621e03a83033531c5\n",
      "  Stored in directory: /private/var/folders/r6/k_cdzkx14flddy2y0tvhd57r0000gn/T/pip-ephem-wheel-cache-nx8sucui/wheels/8c/92/37/ada3fcfdf537bab790219920443164923e6cbfcbd80174af23\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: numpy, diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.27 numpy-1.26.3\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python --no-cache-dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c1bb84-2e7e-4e67-973c-aaa6fc28bc36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main.py', 'data']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./app')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46a1e46f-d527-4091-997b-9d34fbd2341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./app/data/\").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c167c942-aac6-4cb1-a4f0-045162753487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import LangchainEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d113b905-02c9-40e7-a344-ab71a6c0ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import re\n",
    "\n",
    "def check_for_cpt_code(text):\n",
    "    \"\"\"\n",
    "    Checks for the presence of a 5-digit CPT code in a given string.\n",
    "\n",
    "    This function uses regular expressions to search for a 5-digit number (CPT code) in the provided text. \n",
    "    It returns a boolean indicating whether such a code is found and the code itself if present.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The text string to be searched for a CPT code.\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - A boolean indicating whether a 5-digit CPT code is present.\n",
    "        - The CPT code in integer format if present, or None if not present.\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular expression to find 5 consecutive digits\n",
    "    match = re.search(r'\\b\\d{5}\\b', text)\n",
    "\n",
    "    if match:\n",
    "        # Convert the found code to integer\n",
    "        code = int(match.group())\n",
    "        return True, code\n",
    "    else:\n",
    "        return False, None\n",
    "\n",
    "\n",
    "\n",
    "def count_yes_no(strings):\n",
    "    \"\"\"\n",
    "    Counts occurrences of 'Yes' and 'No' within the first five tokens of each string in a list.\n",
    "\n",
    "    This function goes through each string in the provided list, tokenizes the string, and checks\n",
    "    the first five tokens for occurrences of 'Yes' or 'No'. The search is case-insensitive. It counts\n",
    "    the occurrences of each and returns the counts.\n",
    "\n",
    "    Parameters:\n",
    "    strings (list of str): A list of strings to be searched.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two integers:\n",
    "        - The count of 'Yes' occurrences.\n",
    "        - The count of 'No' occurrences.\n",
    "    \"\"\"\n",
    "\n",
    "    yes_count = 0\n",
    "    no_count = 0\n",
    "\n",
    "    for string in strings:\n",
    "        tokens = string.split()[:5]\n",
    "        for token in tokens:\n",
    "            if re.search(r'\\byes\\b', token, re.IGNORECASE):\n",
    "                yes_count += 1\n",
    "                break\n",
    "            elif re.search(r'\\bno\\b', token, re.IGNORECASE):\n",
    "                no_count += 1\n",
    "                break\n",
    "\n",
    "    return yes_count, no_count\n",
    "\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_age(dob_string, reference_date=None):\n",
    "    \"\"\"\n",
    "    Calculate the age of a patient based on their date of birth extracted from a given string.\n",
    "\n",
    "    Parameters:\n",
    "    dob_string (str): A string containing the patient's date of birth in mm/dd/yyyy format.\n",
    "    reference_date (str, optional): The date from which to calculate the age, in mm/dd/yyyy format.\n",
    "                                   Defaults to the current date if not provided.\n",
    "\n",
    "    Returns:\n",
    "    int: The age of the patient.\n",
    "    \"\"\"\n",
    "    # Regular expression to find the date of birth in the string\n",
    "    dob_match = re.search(r'\\b(\\d{2}/\\d{2}/\\d{4})\\b', dob_string)\n",
    "\n",
    "    if not dob_match:\n",
    "        raise ValueError(\"Date of birth not found in the provided string.\")\n",
    "\n",
    "    dob = datetime.strptime(dob_match.group(), '%m/%d/%Y')\n",
    "\n",
    "    # Use the current date or the provided reference date to calculate the age\n",
    "    if reference_date:\n",
    "        reference_date = datetime.strptime(reference_date, '%m/%d/%Y')\n",
    "    else:\n",
    "        reference_date = datetime.today()\n",
    "\n",
    "    # Calculate age\n",
    "    age = reference_date.year - dob.year - ((reference_date.month, reference_date.day) < (dob.month, dob.day))\n",
    "\n",
    "    return age\n",
    "\n",
    "def detect_first_degree_relative(text):\n",
    "    \"\"\"\n",
    "    Detects if a string contains any mention of a first-degree relative and identifies the relative.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input string to be searched.\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - A boolean indicating whether a first-degree relative is mentioned.\n",
    "        - The first identified first-degree relative or None if none is mentioned.\n",
    "    \"\"\"\n",
    "    # Define a list of first-degree relatives\n",
    "    relatives = [\"mother\", \"father\", \"brother\", \"sister\", \"son\", \"daughter\"]\n",
    "\n",
    "    # Create a regular expression pattern to find these relatives\n",
    "    pattern = r'\\b(?:' + '|'.join(relatives) + r')\\b'\n",
    "\n",
    "    # Search the text for the pattern\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    # Check if a match is found and return the results\n",
    "    if match:\n",
    "        return True, match.group().lower()\n",
    "    else:\n",
    "        return False, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5568cf5-dd7a-46f9-a998-27a8c2a9594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading url https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf to path /Users/jasperhajonides/Library/Caches/llama_index/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\n",
      "total size (MB): 5131.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4894it [01:24, 57.74it/s]                                                         \n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/jasperhajonides/Library/Caches/llama_index/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4893.70 MiB, ( 4893.77 / 10922.67)\n",
      "llm_load_tensors: system memory used  = 4893.10 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: ggml.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/jasperhajonides/miniconda3/envs/medical_assessment/lib/python3.10/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   487.50 MiB, ( 5382.83 / 10922.67)\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 5382.84 / 10922.67)\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 278.56 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   275.38 MiB, ( 5658.20 / 10922.67)\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n",
      "Load pretrained SentenceTransformer: thenlper/gte-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b19b6dd40343afb438fd80fc9df449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acfc186ba0849b29d3bcd736eb40c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ac81cdcc144c1ea337294a6925619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/67.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b10c7da9b54405ad1beda3aed7719f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cbf57621b5406aa3c10e1a294f3cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab584cf4e0249668ff8e00dd99c33bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42544dcc8154013b92f4ac623e2644c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908ad0fd14c04b189527f172a13abc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31399424d7164a23adde935d860b7b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458249913b96459f86d934f867c7e87b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52795c1e0b1f494a820a1ff8927db0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "onnx/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68e57d2f8a64ed0989b48b165925895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/670M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f40fd8ab4b945b29be0d6911df666f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e417a5279508422f8f6919133a233a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23de63c5f1334c20b3efbc5701ddf390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab0578a875f4a74ade2b435c3fb2da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d690123daa44a79a61604d25095284d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9697997c47840c4b40393c226d9d6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "Use pytorch device: cpu\n",
      "Use pytorch device: cpu\n",
      "Use pytorch device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%time` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Load documents\n",
    "documents = SimpleDirectoryReader(\"./app/data/\").load_data()\n",
    "\n",
    "# Initialize the LlamaCPP model\n",
    "llm = LlamaCPP(\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf',\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Initialize the embedding model\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"thenlper/gte-large\")\n",
    ")\n",
    "\n",
    "# Set up the service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Measure execution time\n",
    "%%time\n",
    "\n",
    "# Create an index from documents\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Perform a query and print the response\n",
    "response = query_engine.query(\"who founded dunnhumby?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a63cfb74-c0ec-4bf8-82b1-ed0ef335924b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a34aef2a3214db3849bd11b8b3ac25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464060dfdeba49fbb98d5d83b3cb377e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context information, the diagnosis for James Maddison is internal hemorrhoids.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      18.69 ms /    22 runs   (    0.85 ms per token,  1177.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17856.35 ms /   914 tokens (   19.54 ms per token,    51.19 tokens per second)\n",
      "llama_print_timings:        eval time =    2078.17 ms /    21 runs   (   98.96 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:       total time =   20120.35 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an index from documents\n",
    "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Perform a query and print the response\n",
    "response = query_engine.query(\"what is the diagnosis? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45c1d84c-6360-4462-ad38-8757a5da4ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc1914781e64b6688a2be2a2986c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, I can help with that. Based on the context information provided, there is one CPT code mentioned which is 45378. This code represents a colonoscopy procedure.\n",
      "CPT code present: True\n",
      "CPT code: 45378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      14.74 ms /    41 runs   (    0.36 ms per token,  2781.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    4203.42 ms /    41 runs   (  102.52 ms per token,     9.75 tokens per second)\n",
      "llama_print_timings:       total time =    4399.59 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "response = query_engine.query(\"Hospitals use CPT codes to treatments, these codes are 5 digits. Can you identify any of these codes related to treatments in this report?\")\n",
    "text = response.response\n",
    "code_present, code = check_for_cpt_code(text)\n",
    "\n",
    "print(f\"CPT code present: {code_present}\")\n",
    "print(f\"CPT code: {code if code is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78edab8f-f41e-4543-a556-864a639548bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212ed049d3534f7cb725b9a49a3013e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The requested procedure for this report is 45378, which represents a standard diagnostic colonoscopy.\n",
      "Found code: 45378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      16.41 ms /    24 runs   (    0.68 ms per token,  1462.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5722.14 ms /   479 tokens (   11.95 ms per token,    83.71 tokens per second)\n",
      "llama_print_timings:        eval time =    2237.15 ms /    23 runs   (   97.27 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =    8139.46 ms\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the requested procedure for this report?\")\n",
    "print(response)\n",
    "\n",
    "import re\n",
    "text = response.response\n",
    "# Regular expression to find 5 consecutive digits\n",
    "match = re.search(r'\\b\\d{5}\\b', text)\n",
    "\n",
    "if match:\n",
    "    code = match.group()\n",
    "    print(f\"Found code: {code}\")\n",
    "else:\n",
    "    print(\"No 5-digit code found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88b4a0ab-cc4a-4528-bc1b-e4a1a53fccf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7267a44050e4cc7803bfaec9f1855cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1445.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10244.25 ms /   804 tokens (   12.74 ms per token,    78.48 tokens per second)\n",
      "llama_print_timings:        eval time =      98.56 ms /     1 runs   (   98.56 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =   10369.65 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8356205e6c344fcdb8ecc57e5635fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.54 ms /     3 runs   (    0.51 ms per token,  1946.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     299.00 ms /     3 runs   (   99.67 ms per token,    10.03 tokens per second)\n",
      "llama_print_timings:       total time =     318.04 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d304e585027461b8a6809eadabd046f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     2 runs   (    0.24 ms per token,  4149.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     202.19 ms /     2 runs   (  101.10 ms per token,     9.89 tokens per second)\n",
      "llama_print_timings:       total time =     208.37 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaf88c52e1b40fb96a25b0f8479be02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.50 ms /     2 runs   (    0.25 ms per token,  4032.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     202.39 ms /     2 runs   (  101.20 ms per token,     9.88 tokens per second)\n",
      "llama_print_timings:       total time =     210.51 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbaf71420f3408b8b3b9608f700ef5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.02 ms /     3 runs   (    0.34 ms per token,  2955.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     304.98 ms /     3 runs   (  101.66 ms per token,     9.84 tokens per second)\n",
      "llama_print_timings:       total time =     318.24 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a398a61591874901bd1bff7a3ea11c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.72 ms /     2 runs   (    0.36 ms per token,  2758.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     199.04 ms /     2 runs   (   99.52 ms per token,    10.05 tokens per second)\n",
      "llama_print_timings:       total time =     209.45 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d4561059f64d9aa758bf4354c4bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     2 runs   (    0.23 ms per token,  4357.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     206.19 ms /     2 runs   (  103.09 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =     214.53 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7d8831209c4050b4a68bad302ec567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     2 runs   (    0.32 ms per token,  3100.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     196.68 ms /     2 runs   (   98.34 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =     205.90 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9290e40bc4ad46c1874e62003d1cb702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.84 ms /     2 runs   (    0.42 ms per token,  2375.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     201.56 ms /     2 runs   (  100.78 ms per token,     9.92 tokens per second)\n",
      "llama_print_timings:       total time =     211.73 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241eb93ebb75497baf87659e2f170de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      21.19 ms /    34 runs   (    0.62 ms per token,  1604.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3359.99 ms /    34 runs   (   98.82 ms per token,    10.12 tokens per second)\n",
      "llama_print_timings:       total time =    3617.76 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3330781b594ff5bf8d5354a6b04647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.67 ms /     2 runs   (    0.33 ms per token,  2998.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     200.15 ms /     2 runs   (  100.07 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =     207.98 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94dd9de675f42de95cc36db9c90d38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      72.84 ms /   109 runs   (    0.67 ms per token,  1496.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   10912.32 ms /   109 runs   (  100.11 ms per token,     9.99 tokens per second)\n",
      "llama_print_timings:       total time =   11696.06 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391213f47e724f40a6e419d0542ffcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.02 ms /     3 runs   (    0.34 ms per token,  2938.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     304.49 ms /     3 runs   (  101.50 ms per token,     9.85 tokens per second)\n",
      "llama_print_timings:       total time =     318.75 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafa0a4dc265467d872a565f010f498b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /     3 runs   (    0.44 ms per token,  2257.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     296.99 ms /     3 runs   (   99.00 ms per token,    10.10 tokens per second)\n",
      "llama_print_timings:       total time =     312.31 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0aef65526d41d4830196142610c6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Yes' count: 0\n",
      "'No' count: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.55 ms /     2 runs   (    0.27 ms per token,  3663.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     201.28 ms /     2 runs   (  100.64 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =     211.96 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checks = []\n",
    "for i in range(15):\n",
    "    response = query_engine.query(\"Has there been a previous treatment that successfully improved colonalrectal or absominal discomfort? Answer with a yes or a no\")\n",
    "    checks.append(response.response)\n",
    "yes_count, no_count = count_yes_no(checks)\n",
    "print(f\"'Yes' count: {yes_count}\")\n",
    "print(f\"'No' count: {no_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2844640-2310-41a9-8d9d-ae5d58d28954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a5871e3757404893ace36f8f36ce1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      19.57 ms /    29 runs   (    0.67 ms per token,  1482.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4718.74 ms /   438 tokens (   10.77 ms per token,    92.82 tokens per second)\n",
      "llama_print_timings:        eval time =    2799.90 ms /    28 runs   (  100.00 ms per token,    10.00 tokens per second)\n",
      "llama_print_timings:       total time =    7750.70 ms\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"\n",
    "We'd like to check if hte patient had Juvenile polyposis syndrome diagnosis indicated by 1 or more of the following:\n",
    "- Age 12 years or older and symptomatic(eg, abdominalpain, iron deficiency anemia, rectal bleeding,\n",
    "telangiectasia)\n",
    "- Younger than 12 years and symptomatic(eg, abdominalpain, iron deficiency anemia, rectal bleeding,\n",
    "telangiectasia)\n",
    "\n",
    "Is there any indication? Yes or no?\n",
    "\"\"\")\n",
    "print(response.response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7189eff7-91b2-4046-b914-2fe8e2eb26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5cb861b4f84e32aab352a531925fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      30.27 ms /    45 runs   (    0.67 ms per token,  1486.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8191.06 ms /   792 tokens (   10.34 ms per token,    96.69 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.60 ms /    44 runs   (   99.24 ms per token,    10.08 tokens per second)\n",
      "llama_print_timings:       total time =   12960.64 ms\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"What is the date of birth of the patient in this report?\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "32598b3b-0ec9-4504-b3ec-ad29aa605375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient's age: 41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "dob_string = response.response\n",
    "age = calculate_age(dob_string)\n",
    "# Calculate age as of today, can also insert the date at which the report was created in calculate_age(). \n",
    "print(f\"Patient's age: {calculate_age(dob_string)}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fccabec6-02ff-4c38-8bc3-7285af339440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'41 ( The date of birth for the patient in this report is 06/16/1982 for James Freeman and 03/15/1965 for James Maddison.)'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(age) + ' (' + dob_string +')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5304b9d1-bdf2-489f-bb6c-76124be2d32d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c90c125-249e-4d54-865c-fc6abc4f9839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb4c31e175948439603ad2147e86dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      56.46 ms /    81 runs   (    0.70 ms per token,  1434.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     853.42 ms /    40 tokens (   21.34 ms per token,    46.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7709.97 ms /    80 runs   (   96.37 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =    9211.11 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83df6715d57d44f2965932fed4bf5654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      25.91 ms /    40 runs   (    0.65 ms per token,  1543.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3862.91 ms /    40 runs   (   96.57 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =    4148.94 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e59d88338748129c3a38fa6cb49872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      29.46 ms /    40 runs   (    0.74 ms per token,  1357.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3860.77 ms /    40 runs   (   96.52 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =    4170.10 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b444b23c7874c9b879a3ba51c802c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      26.74 ms /    40 runs   (    0.67 ms per token,  1495.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3887.81 ms /    40 runs   (   97.20 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =    4165.71 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f8e003235046c791ef33f73e569f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      28.44 ms /    40 runs   (    0.71 ms per token,  1406.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3854.97 ms /    40 runs   (   96.37 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =    4166.26 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835c36292e3f4811aa4d686b7d4c416c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      26.86 ms /    40 runs   (    0.67 ms per token,  1489.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3872.40 ms /    40 runs   (   96.81 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:       total time =    4163.73 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cf48f319e34c6989e615a0433dd716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      23.62 ms /    31 runs   (    0.76 ms per token,  1312.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2985.70 ms /    31 runs   (   96.31 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =    3223.57 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52079241dc804de8851ac630e0aad747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      30.67 ms /    41 runs   (    0.75 ms per token,  1336.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3962.94 ms /    41 runs   (   96.66 ms per token,    10.35 tokens per second)\n",
      "llama_print_timings:       total time =    4269.74 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ec5ecf5e8c40b7a87e9bcab9a4cbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      53.23 ms /    79 runs   (    0.67 ms per token,  1484.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    7610.94 ms /    79 runs   (   96.34 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =    8212.73 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b40ec0beff458c88a50d7cffa45ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      27.65 ms /    40 runs   (    0.69 ms per token,  1446.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3877.39 ms /    40 runs   (   96.93 ms per token,    10.32 tokens per second)\n",
      "llama_print_timings:       total time =    4164.18 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991716cc591540f9b9e36033c10fe03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      30.57 ms /    40 runs   (    0.76 ms per token,  1308.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3858.31 ms /    40 runs   (   96.46 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =    4161.21 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ba09e7305647709fd996c4e70f3465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      40.26 ms /    60 runs   (    0.67 ms per token,  1490.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    5782.96 ms /    60 runs   (   96.38 ms per token,    10.38 tokens per second)\n",
      "llama_print_timings:       total time =    6240.54 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99e6347433446d08d92c7ad24b8963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      28.32 ms /    40 runs   (    0.71 ms per token,  1412.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3843.47 ms /    40 runs   (   96.09 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =    4152.80 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55013d8bca2a45c3aafcd2e1c1f50f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      26.83 ms /    40 runs   (    0.67 ms per token,  1490.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3846.70 ms /    40 runs   (   96.17 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =    4143.78 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc22f4fed0d4cac80e7a29590fc7571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Yes' count: 0\n",
      "'No' count: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      27.42 ms /    40 runs   (    0.69 ms per token,  1458.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3859.79 ms /    40 runs   (   96.49 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =    4152.75 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checks = []\n",
    "for i in range(5):\n",
    "    response = query_engine.query(\"Check if patient already had a colonoscopy in past 10 years, apart from one that is possible scheduled, yes or no?\")\n",
    "    checks.append(response.response)\n",
    "yes_count, no_count = count_yes_no(checks)\n",
    "print(f\"'Yes' count: {yes_count}\")\n",
    "print(f\"'No' count: {no_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64da8314-c786-48bd-89a6-e45a1ddf15c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec4b48947e7484ba59903f9c324f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       2.76 ms /     3 runs   (    0.92 ms per token,  1086.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10478.48 ms /   866 tokens (   12.10 ms per token,    82.65 tokens per second)\n",
      "llama_print_timings:        eval time =     202.03 ms /     2 runs   (  101.02 ms per token,     9.90 tokens per second)\n",
      "llama_print_timings:       total time =   10713.64 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1967d857b6b34748934042071897a5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /     3 runs   (    0.36 ms per token,  2793.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     303.25 ms /     3 runs   (  101.08 ms per token,     9.89 tokens per second)\n",
      "llama_print_timings:       total time =     320.54 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc78ab747ccf43418dd8cc9d84a23590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /     3 runs   (    0.54 ms per token,  1864.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     320.96 ms /     3 runs   (  106.99 ms per token,     9.35 tokens per second)\n",
      "llama_print_timings:       total time =     339.61 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24ecef87ced4f97be392d13a5737f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       0.89 ms /     3 runs   (    0.30 ms per token,  3363.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     305.01 ms /     3 runs   (  101.67 ms per token,     9.84 tokens per second)\n",
      "llama_print_timings:       total time =     317.28 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f558dc22a540cd995a194d9c51d264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Yes.', ' Yes.', ' Yes.', ' Yes.', ' Yes.']\n",
      "'Yes' count: 5\n",
      "'No' count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =       1.24 ms /     3 runs   (    0.41 ms per token,  2421.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     296.82 ms /     3 runs   (   98.94 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:       total time =     310.47 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checks = []\n",
    "for i in range(5):\n",
    "    response = query_engine.query(\"Is the patient symptomatic (e.g. abdominal pain, iron deficiency anemia, rectal bleeding)? Answer just yes or no.\")\n",
    "    checks.append(response.response)\n",
    "print(checks)\n",
    "yes_count, no_count = count_yes_no(checks)\n",
    "print(f\"'Yes' count: {yes_count}\")\n",
    "print(f\"'No' count: {no_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7755a984-488d-43be-af96-14151e7ac0ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7235da78964a4a6b8fd7272158d381e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      25.97 ms /    32 runs   (    0.81 ms per token,  1232.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1958.17 ms /    33 tokens (   59.34 ms per token,    16.85 tokens per second)\n",
      "llama_print_timings:        eval time =    3063.94 ms /    31 runs   (   98.84 ms per token,    10.12 tokens per second)\n",
      "llama_print_timings:       total time =    5298.15 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd1d5de45334bd6a24c91bcf1c61f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      24.36 ms /    32 runs   (    0.76 ms per token,  1313.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3165.18 ms /    32 runs   (   98.91 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:       total time =    3411.96 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e33fb35cd2d471497b3f9f867db6f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      24.31 ms /    32 runs   (    0.76 ms per token,  1316.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3168.37 ms /    32 runs   (   99.01 ms per token,    10.10 tokens per second)\n",
      "llama_print_timings:       total time =    3409.14 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7512541fb1df451e8969f55a38dba049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      13.28 ms /    21 runs   (    0.63 ms per token,  1581.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    2097.53 ms /    21 runs   (   99.88 ms per token,    10.01 tokens per second)\n",
      "llama_print_timings:       total time =    2249.38 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8124dd0046d4efb96faf97b5d16d582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n",
      "[\" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, James Freeman's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\"]\n",
      "[True, True, True, True, True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   13681.85 ms\n",
      "llama_print_timings:      sample time =      19.47 ms /    32 runs   (    0.61 ms per token,  1643.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    3220.88 ms /    32 runs   (  100.65 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =    3413.14 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "checks = []\n",
    "relative_yn = []\n",
    "for i in range(5):\n",
    "    response = query_engine.query(\"Is there any family history of colorectal cancer? If yes, answer just with the family relationship\")\n",
    "    checks.append(response.response)\n",
    "\n",
    "    relative_present, relative = detect_first_degree_relative(response.response)\n",
    "    relative_yn.append(relative_present)\n",
    "    print(f\"First-degree relative present: {relative_present}\")\n",
    "    print(f\"Identified relative: {relative if relative else 'None'}\")\n",
    "\n",
    "print(checks)\n",
    "print(relative_yn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f111438-2191-4367-90fa-2106752d2d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfb7fef1-29ad-4c3c-806a-7598319ced00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-degree relative present: True\n",
      "Identified relative: father\n"
     ]
    }
   ],
   "source": [
    "checks = [\" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, James Freeman's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\", \" Yes, there is a family history of colorectal cancer. The patient's father had colorectal cancer at age 68.\"]\n",
    "# Example usage\n",
    "text = checks[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f88f7-c2a5-45d2-be09-516379f2fe02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
